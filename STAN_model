import numpy as np
import sys
import torch
import torch.nn as nn
from torch.distributions import Normal, OneHotCategorical
import torch.nn.functional as F ## necessary?
import pandas as pd
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")



#### Need: a GMM network to decode cont. vars, a softmax if we have categorical,
#### losses for both cases (negative log likelihood for the cont. case; cross entropy for the categorical case),
#### construct the window data and get the wanted row as labels, 
class wCnn(nn.Module):
    ## the window-CNN which learns the temporal and inter-attribute relations
    ##
    ##
    ##
    def __init__(self, input_dim,output_dim,decoder_architecture,decoder=None,num_components=1):
        super().__init__()
        self.input_dim=input_dim
        
        self.conv64 = nn.Conv2d(1, 64, kernel_size=3, padding=1)
        self.conv128 = nn.Conv2d(1, 128, kernel_size=3, padding=1)
        self.bn64= nn.BatchNorm2d(64)
        self.bn128= nn.BatchNorm2d(128)
        self.relu=nn.ReLU(inplace=True)
        self.maxpool=nn.MaxPool2d(kernel_size=2, stride=2)
        self.decoder_architecture=decoder_architecture
        assert self.decoder_architecture in ['gmm','softmax'], "Decoder not recognised"
        ## I think this is the batch size
        decoder_input_dim=256
        if self.decoder_architecture=='gmm':
            ## create the gmm layers
            self.decoder=mixtureDensityNetwork(decoder_input_dim,output_dim,num_components)
        else:
            self.decoder=softmaxNetwork(input_dim,output_dim)
        
    def forward(self, x):
        ## here we should give it to the mask first
        x=make_mask(x)
        ## Base CNN-layout: 64 3x3, BN, ReLU, 64 3x3, BN, ReLU, MaxPool, 128 3x3, BN, ReLU, 128 3x3, BN, ReLU, MaxPool
        
        x = helper(self,x,64)
        x = helper(self,x,64)
        x = self.maxpool(x)
        x = helper(self,x,128)
        x = helper(self,x,128)
        x = self.maxpool(x)
        ### pass it to either decoder part after this
        results=self.decoder.forward(x)
        return results
    def make_mask(self, x):
        
        # print('masking', x, self.dim_in)
        x = x[:, :-self.dim_in]
           
        #x = x * self.mask.to(device)
        return x    
        
        return x
    def helper(self,x,size):
        if size==64:
            x = self.conv64(x)
            x = self.bn64(x)
            x = self.relu(x)
        elif size==128:
            x = self.conv128(x)
            x = self.bn128(x)
            x = self.relu(x)
        else:
            print("Size "+str(size)+" not supported!\n")
            sys.exit(-1)
        return x
    
    def loss(self, x, y, binary=False):
        if self.decoder == 'gmm':
            pi, normal = self.forward(x)
            '''
            if binary:
                batch_loss = #self.gmm_network.binary_loss(pi, normal, y)
            else:
                batch_loss = #self.gmm_network.loss(pi, normal, y)
            '''
        else:
            ## we do softmax as decoder
            out = self.forward(x)
            if y.size()[1] > 1:
                y = torch.max(y, 1)[1].long()
               
            batch_loss = torch.nn.CrossEntropyLoss()(out, y)          
        #self.memory.append(batch_loss.cpu().detach().numpy())
        return batch_loss

class mixtureDensityNetwork(nn.Module):
    # three parallel FC layers which model \alpha_i, \mu_i and \sigma_i resp.
    # \alpha_i go to softmax s.t \sum_i \alpha_i=1
    
    def __init__(self,input_dim,output_dim, num_mixture_components):
        self.input_dim=input_dim
        self.num_components=num_mixture_components
        self.output_dim=output_dim
        
class GMMNet(nn.Module):
    ### here we define the GMM-layer that should learn one part of the parameters in mixtureDensityNetwork
    def __init__(self,input_dim,output_dim,num_components):
        super().__init__()
        self.input_dim=input_dim
        self.output_dim=output_dim
        self.num_component=num_component
    def forward():
        print("TODO")
class softmaxNetwork(nn.Module):
    def __init__(self,input_dim,output_dim,hidden_dim=100):
        super().__init__()
        self.input_dim=input_dim
        self.output_dim=output_dim
        self.hidden_dim=hidden_dim
         
    def forward(self,x):
        dec_layer = nn.Sequential(
                nn.Linear(self.input_dim, self.hidden_dim),
                nn.ReLU(inplace=True),
                nn.Linear(self.hidden_dim, self.output_dim),
                nn.ReLU(inplace=True)
            )
        res=dec_layer(x)
        return res
        
        
        
        
        
from keras.utils import to_categorical
def synthesizer(historical_data,categorical_columns=[5,10,11,12,13],continuous_columns=[1,4,6,7,8,15],numeric_columns=[0, 2, 3, 9, 14, 16, 17, 18]):
    
    ### we take out the categorical, numeric/integer and continuous variables; we preprocess them
    ### and put them into lists. Thus we can go through and train for each attribute.
    for i in categorical_columns:
        disc_data = historical_data.iloc[:, i]
        disc_data = to_categorical(disc_data)
        #print(disc_data)
        ## a note here is that most of the categorical variables are constant
    for i in continuous_columns:
        cont_data = historical_data.iloc[:, i]
    for i in numeric_columns:
        num_data = historical_data.iloc[:, i]
        num_data = (num_data-np.amin(num_data))/(np.amax(num_data)-np.amin(num_data))
    
    wCnn(1,2,'softmax')
    ### Here we pass each attribute into 
    
    ### We therefore save a model after training on each attribute.
    
data=pd.read_csv("~/Downloads/one_PatientADNI.csv")
synthesizer(data)